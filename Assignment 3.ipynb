{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd595e9",
   "metadata": {},
   "source": [
    "## Assignment 3 - PyTorch version\n",
    "\n",
    "This is a version of assignment 3 that is made in [PyTorch](https://pytorch.org/) rather than Tensorflow/Keras.\n",
    "\n",
    "This is for anyone who has been having trouble running tensorflow on their computer (especially if you have an M2 Mac!), or for students who just generally want the experience of working with a different deep learning library! For several years now PyTorch has become the most [commonly used library by researchers](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2023/) in this field.\n",
    "\n",
    "PyTorch operates at a lower level of abstraction to Keras. Which means we need to write a bit more code to implement and train our model. This may make the code look more complex at first, but actually this is often preferable as it is hiding less about what the code is doing from you and you have more control over how your models are built, trained and represented.\n",
    "\n",
    "If you want more practice with PyTorch [take a look at their excellent set of tutorials](https://pytorch.org/tutorials/).\n",
    "\n",
    "- __Extend the model in this notebook into one which maps (X,Y) -> (R,G,B).__\n",
    "- __Add at least 2 more layers to the network.__\n",
    "- __Experiment with alternative activation functions and optimizers.__\n",
    "- __In a paragraph or so, describe how the image we have created differs from a normal image.__\n",
    "\n",
    "You can find other images to play with [from SciKit Image here](https://scikit-image.org/docs/dev/api/skimage.data.html), but of course you could experiment with using you're own images. For that you might want to use the [Pillow](https://pillow.readthedocs.io/en/stable/) package which has some [handy functions for loading and manipulating images](https://pillow.readthedocs.io/en/stable/reference/Image.html).\n",
    "\n",
    "This shouldn't take you longer than an afternoon! __This will be handed in at the end of the module__ so once you have something working it would be _much appreciated_ if you go back over your code and tidy it up, maybe add comments to describe what is happening in the code.\n",
    "\n",
    "Here are some more lovely examples from [David Ha](https://twitter.com/hardmaru):\n",
    "\n",
    "![David Ha bw](./images/hardmaru_color.png)\n",
    "\n",
    "---\n",
    "\n",
    "If you like this work you could take some ideas explored by David Ha in his blog posts on this topic and re-implement them, or take them further for your final project. I think there is a lot of potential for creating really interesting images and even interesting drawing tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import skimage\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbb418",
   "metadata": {},
   "source": [
    "Set the device for your machine.\n",
    "\n",
    "For Mac M1/M2 this should say device should be `mps`.\n",
    "\n",
    "For a machine with an NVIDIA GPU device should be `cuda`.\n",
    "\n",
    "To run on the CPU of any other machine, device should be `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda'\n",
    "device = 'mps' \n",
    "#device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3aa6f579",
   "metadata": {},
   "source": [
    "The below code is the original code provided in the Assignment brief. The rest of the assignment has been broken up into different sections, as follows:\n",
    "\n",
    "Section 1 - The original image and model provided in the assignment notebook. \n",
    "Section 2 - Extending the model to match (X,Y) to (R, G, B).\n",
    "Section 3 - Adding two or more layers to the neural network.\n",
    "Section 4 - Experimenting with different activation functions (4.1) and optimizers (4.2).\n",
    "Section 5 - In a paragraph or so, describe how the image we have created differs from a normal image. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0ed56a0",
   "metadata": {},
   "source": [
    "<font size = \"5\"> Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1681ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get image from skimage\n",
    "img = skimage.data.camera()\n",
    "smaller_img = resize(img, (64, 64)) # Resize it just to make things quicker\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b059947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our function that gets a grid of coordinates\n",
    "def get_mgrid(sidelen):\n",
    "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.'''\n",
    "    width = np.linspace(-1, 1, sidelen)\n",
    "    height = np.linspace(-1, 1, sidelen)\n",
    "    mgrid = np.stack(np.meshgrid(width, height), axis=-1)\n",
    "    mgrid = np.reshape(mgrid, [-1, 2])\n",
    "    return mgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fad6d8",
   "metadata": {},
   "source": [
    "Defining the variables we will be using as our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b65542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the variables for training\n",
    "\n",
    "image_side_length = smaller_img.shape[0]\n",
    "X = get_mgrid(image_side_length)      # Inputs\n",
    "X = np.float32(X)\n",
    "\n",
    "#Convert input array to torch tensor\n",
    "X_torch_tensor = torch.tensor(X, device=device)\n",
    "\n",
    "y = np.reshape(smaller_img, [-1, 1])  # Outputs\n",
    "y = np.float32(y)\n",
    "\n",
    "#convert output array to torch tensor\n",
    "y_torch_tensor = torch.tensor(y, device=device)\n",
    "\n",
    "#Get the total number of coordinates \n",
    "num_coords = X_torch_tensor.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0759a91",
   "metadata": {},
   "source": [
    "Lets check to see our data tensor is running on the right device.\n",
    "\n",
    "For Mac M1/M2 this should say `mps:0`.\n",
    "\n",
    "For CPU this should say `cpu`.\n",
    "\n",
    "For a machine with an NVIDIA GPU this should say `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_torch_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52906ed",
   "metadata": {},
   "source": [
    "Here we define our model. Our network is a class that inherits from the base model in tensorflow `nn.Module`.\n",
    "\n",
    "`def __init__(self):` Is where the constructor is defined, this is the function that gets called when our network is first **initialised** here we tell it what the layers are and the parameters that we want to keep track of.\n",
    "\n",
    "`def forward(self, x):` is where we define the forward pass of the model. This is what we want the model to do with every input data example and define what layers (and activation functions) are ran, and what the output is that our model generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPPN(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(CPPN, self).__init__()\n",
    "\n",
    "      # First fully connected layer\n",
    "      self.fc1 = nn.Linear(2, 16)\n",
    "      # Second fully connected layer\n",
    "      self.fc2 = nn.Linear(16, 32)\n",
    "      # Third fully connected layer\n",
    "      self.fc3 = nn.Linear(32, 1)     \n",
    "    \n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass through first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Pass through second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass through third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        # Sigmoid activation function\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        # Return our output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c89ae6",
   "metadata": {},
   "source": [
    "This is the code where we create our model, we create one version of our network based on our `CPPN` class. We also need to define the loss function we are using `criterion` and what `optimiser` we are using for updating the weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f08038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our model \n",
    "\n",
    "num_epochs = 8\n",
    "batch_size = 1\n",
    "\n",
    "cppn = CPPN()\n",
    "cppn.to(device)\n",
    "cppn.requires_grad_()\n",
    "\n",
    "optimiser = torch.optim.SGD(cppn.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd7bd8",
   "metadata": {},
   "source": [
    "This is wheere we train the model. We have a for loop that tells us how many times we want to process data during training. This depends on the size of the data `num_coords`, the number of epochs (cycles through the data) `num_epochs`, and the batch size `batch_size`.\n",
    "\n",
    "During training we process one batch at a time, our input batch is the `input_batch` variable (which is our batch of coordinates), our output batch is the `true_pixel_values`. \n",
    "\n",
    "We use our network model to get approximations of the pixel values `approx_pixel_values` based on the input coordinates, and compare that to our true pixel values using our loss function critera. Once we have our the value of our loss for that batch `loss`, we call `loss.backward()` to backpropagate the error through the network model. We then call `optimiser.step()` to update the weights of our neural network model. \n",
    "\n",
    "We do this repeatedly until we have completed the number of cycles through our datset `num_epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71784e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_indexes = list(range(0, num_coords))\n",
    "for i in range( int((num_epochs * num_coords) / batch_size) ):\n",
    "    optimiser.zero_grad()\n",
    "    cppn.zero_grad()\n",
    "\n",
    "    # Get batch of data\n",
    "    batch_indexes = torch.tensor(np.array(random.sample(coord_indexes, batch_size)))\n",
    "    input_batch = X_torch_tensor[batch_indexes]\n",
    "    true_pixel_values = y_torch_tensor[batch_indexes]\n",
    "\n",
    "    # Process data with model\n",
    "    approx_pixel_values = cppn(input_batch)\n",
    "\n",
    "    # Calculate loss function\n",
    "    loss = criterion(true_pixel_values, approx_pixel_values)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f'step {i}, loss {loss:.3f}')\n",
    "    \n",
    "    #Update model\n",
    "    loss.backward()\n",
    "    optimiser.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f915e",
   "metadata": {},
   "source": [
    "Now lets process the whole image again with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ff210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the image \n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = cppn(X_torch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e8eb9",
   "metadata": {},
   "source": [
    "Now lets visualise the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape it from 1D to 2D\n",
    "reconstructed_img = np.reshape(prediction.cpu(), (64, 64))\n",
    "# Scale the values from [0,1] to [0, 255]\n",
    "reconstructed_img *= 255\n",
    "# Convert the tensor into a numpy array, and cast the type into a uint8.\n",
    "reconstructed_img = reconstructed_img.numpy().astype(np.uint8)\n",
    "# Look at our creation next to the original!\n",
    "fig, axes_array = plt.subplots(1,3, figsize=(20,10))\n",
    "axes_array[0].imshow(img, cmap='gray')\n",
    "axes_array[1].imshow(smaller_img, cmap='gray')\n",
    "axes_array[2].imshow(reconstructed_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4daf9d7",
   "metadata": {},
   "source": [
    "<font size = \"5\" > Section 2 - Extend the Model to map (X,Y) -> (R,G,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1988698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get image from skimage\n",
    "\n",
    "img_color = skimage.data.astronaut()\n",
    "smaller_img_color = resize(img, (64, 64)) # Resize it just to make things quicker\n",
    "plt.imshow(img_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function that gets a grid of coordinates\n",
    "\n",
    "def get_mgrid(sidelen):\n",
    "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.'''\n",
    "    width = np.linspace(-1, 1, sidelen)\n",
    "    height = np.linspace(-1, 1, sidelen)\n",
    "    mgrid = np.stack(np.meshgrid(width, height), axis=-1)\n",
    "    mgrid = np.reshape(mgrid, [-1, 2])\n",
    "    return mgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the variables for training\n",
    "\n",
    "image_side_length_color = smaller_img_color.shape[0]\n",
    "X = get_mgrid(image_side_length_color)      # Inputs\n",
    "X = np.float32(X)\n",
    "\n",
    "#Convert input array to torch tensor\n",
    "X_torch_tensor = torch.tensor(X, device=device)\n",
    "\n",
    "y = np.reshape(smaller_img_color, [-1, 3])  # Outputs - adjusted the outer limit to be sure 3 outputs are generated. \n",
    "y = np.float32(y)\n",
    "\n",
    "#convert output array to torch tensor\n",
    "y_torch_tensor = torch.tensor(y, device=device)\n",
    "\n",
    "#Get the total number of coordinates \n",
    "num_coords = X_torch_tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c99165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a CPPN network to map color images\n",
    "\n",
    "class CPPN_color(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(CPPN_color, self).__init__()\n",
    "\n",
    "      # First fully connected layer\n",
    "      self.fc1 = nn.Linear(2, 16)\n",
    "      # Second fully connected layer\n",
    "      self.fc2 = nn.Linear(16, 32)\n",
    "      # Third fully connected layer\n",
    "      self.fc3 = nn.Linear(32, 3)     #adjusted the output of the layer to produce 3 values rather than 1 as RGB requires 3 values\n",
    "    \n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass through first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Pass through second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass through third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        # Sigmoid activation function\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        # Return our output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our model to train on the color CPPN defined above\n",
    "\n",
    "num_epochs = 20 #increased the number of epochs to better refine results (less epochs weren't as interesting)\n",
    "batch_size = 1\n",
    "\n",
    "cppnc = CPPN_color()\n",
    "cppnc.to(device)\n",
    "cppnc.requires_grad_()\n",
    "\n",
    "optimiser = torch.optim.SGD(cppnc.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c63ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the color model \n",
    "\n",
    "coord_indexes = list(range(0, num_coords))\n",
    "for i in range( int((num_epochs * num_coords) / batch_size) ):\n",
    "    optimiser.zero_grad()\n",
    "    cppnc.zero_grad()\n",
    "\n",
    "    # Get batch of data\n",
    "    batch_indexes = torch.tensor(np.array(random.sample(coord_indexes, batch_size)))\n",
    "    input_batch = X_torch_tensor[batch_indexes]\n",
    "    true_pixel_values = y_torch_tensor[batch_indexes]\n",
    "\n",
    "    # Process data with model\n",
    "    approx_pixel_values = cppnc(input_batch)\n",
    "\n",
    "    # Calculate loss function\n",
    "    loss = criterion(true_pixel_values, approx_pixel_values)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f'step {i}, loss {loss:.3f}')\n",
    "    \n",
    "    #Update model\n",
    "    loss.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the image\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction_color = cppnc(X_torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring and diplaying the results of the color image after it has been trained on the model \n",
    "\n",
    "# Reshape it from 1D to 2D\n",
    "reconstructed_img_color = np.reshape(prediction_color.cpu(), (64, 64, 3)) #added in a third dimension to accommodate for the 3 RGB values\n",
    "# Scale the values from [0,1] to [0, 255]\n",
    "reconstructed_img_color *= 255\n",
    "# Convert the tensor into a numpy array, and cast the type into a uint8.\n",
    "reconstructed_img_color = reconstructed_img_color.numpy().astype(np.uint8)\n",
    "# Look at our creation next to the original!\n",
    "fig, axes_array = plt.subplots(1,3, figsize=(20,10))\n",
    "axes_array[0].imshow(img_color)\n",
    "axes_array[1].imshow(smaller_img_color)\n",
    "axes_array[2].imshow(reconstructed_img_color) \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11435997",
   "metadata": {},
   "source": [
    "For this section, I adjusted the model to map (X,Y) to (R, G, B) by adjusting the Neural Networks outputs (as seen when setting up the layers of the NN and when reshaping the image prior to training) and dimensions of the photograph once the model has been trained. In doing this, the model is now set up to take in 3 color values, as is necessary for an RGB image. The final output of this training is viewable in the above cell, shown by the right most image. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acb82505",
   "metadata": {},
   "source": [
    "<font size = \"5\" > Section 3 - Add at least two layers to the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fd6d7c4",
   "metadata": {},
   "source": [
    "To complete this section, I will be going back to using the original black and white photograph used in Section 1. I have updated the names of the models to reflect the additional layers (and so as not to confuse with the previous code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get image from skimage\n",
    "\n",
    "img_add = skimage.data.camera()\n",
    "smaller_img_add = resize(img_add, (64, 64)) # Resize it just to make things quicker\n",
    "plt.imshow(img_add, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67127f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function that gets a grid of coordinates\n",
    "\n",
    "def get_mgrid(sidelen):\n",
    "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.'''\n",
    "    width = np.linspace(-1, 1, sidelen)\n",
    "    height = np.linspace(-1, 1, sidelen)\n",
    "    mgrid = np.stack(np.meshgrid(width, height), axis=-1)\n",
    "    mgrid = np.reshape(mgrid, [-1, 2])\n",
    "    return mgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ea5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining our variables for training\n",
    "\n",
    "image_side_length_color = smaller_img_color.shape[0]\n",
    "X = get_mgrid(image_side_length_color)      # Inputs\n",
    "X = np.float32(X)\n",
    "\n",
    "#Convert input array to torch tensor\n",
    "X_torch_tensor = torch.tensor(X, device=device)\n",
    "\n",
    "y = np.reshape(smaller_img_color, [-1, 1])  # Outputs\n",
    "y = np.float32(y)\n",
    "\n",
    "#convert output array to torch tensor\n",
    "y_torch_tensor = torch.tensor(y, device=device)\n",
    "\n",
    "#Get the total number of coordinates \n",
    "num_coords = X_torch_tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining our larger neural network \n",
    "\n",
    "class CPPN_add(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(CPPN_add, self).__init__()\n",
    "\n",
    "      # First fully connected layer\n",
    "      self.fc1 = nn.Linear(2, 16)\n",
    "      # Second fully connected layer\n",
    "      self.fc2 = nn.Linear(16, 32)\n",
    "      # Third fully connected layer\n",
    "      self.fc3 = nn.Linear(32, 64) #adjusted the values for the third fully connected layer\n",
    "      # Fourth fully connected layer \n",
    "      self.fc4 = nn.Linear(64, 32) #adjusted the values for the fourth fully connected layer\n",
    "      # Fifth fully connected layer\n",
    "      self.fc5 = nn.Linear(32, 1)    \n",
    "    \n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass through first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Pass through second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass through third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass through fourth fully connected layer\n",
    "        x = self.fc4(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass through fifth fully connected layer\n",
    "        x = self.fc5(x)\n",
    "        # Sigmoid activation function\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        # Return our output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our model and defining the training epochs and batch size\n",
    "\n",
    "num_epochs = 8\n",
    "batch_size = 1\n",
    "\n",
    "cppna = CPPN_add()\n",
    "cppna.to(device)\n",
    "cppna.requires_grad_()\n",
    "\n",
    "optimiser = torch.optim.SGD(cppna.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e32b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model \n",
    "\n",
    "coord_indexes = list(range(0, num_coords))\n",
    "for i in range( int((num_epochs * num_coords) / batch_size) ):\n",
    "    optimiser.zero_grad()\n",
    "    cppna.zero_grad()\n",
    "\n",
    "    # Get batch of data\n",
    "    batch_indexes = torch.tensor(np.array(random.sample(coord_indexes, batch_size)))\n",
    "    input_batch = X_torch_tensor[batch_indexes]\n",
    "    true_pixel_values = y_torch_tensor[batch_indexes]\n",
    "\n",
    "    # Process data with model\n",
    "    approx_pixel_values = cppna(input_batch)\n",
    "\n",
    "    # Calculate loss function\n",
    "    loss = criterion(true_pixel_values, approx_pixel_values)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f'step {i}, loss {loss:.3f}')\n",
    "    \n",
    "    #Update model\n",
    "    loss.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ce889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the image\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction_add = cppna(X_torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2df9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring and diplaying the results of the color image after it has been trained on the model \n",
    "\n",
    "# Reshape it from 1D to 2D\n",
    "reconstructed_img_add = np.reshape(prediction_add.cpu(), (64, 64))\n",
    "# Scale the values from [0,1] to [0, 255]\n",
    "reconstructed_img_add *= 255\n",
    "# Convert the tensor into a numpy array, and cast the type into a uint8.\n",
    "reconstructed_img_add = reconstructed_img_add.numpy().astype(np.uint8)\n",
    "# Look at our creation next to the original!\n",
    "fig, axes_array = plt.subplots(1,3, figsize=(20,10))\n",
    "axes_array[0].imshow(img_add, cmap='gray')\n",
    "axes_array[1].imshow(smaller_img_add, cmap='gray')\n",
    "axes_array[2].imshow(reconstructed_img_add, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a851d82",
   "metadata": {},
   "source": [
    "As shown in the code above, I added two additional layers to my network. Instead of the network mapping to 32 output values and back down, my network went up to 64 outputs, before coming back to 32 and then 1. What is interesting about the output of this network is the difference in color, or truly, the size of the black represented in the image. In the original network, the blurred image recognizes a humanoid shape, acknowledging the area surrounding the man's head which is not of a darker color. Similarly, it doesn't recognize the face or camera as dark either. In this new neural network, however, that delineation seems to have disappeared. The entire left-hand side of the image is now black. It would be interested to continue studying the reason behind this, and ask questions such as could this be a result of having jumped from 32 outputs to 1 so quickly? Would it produce a better result by having a slower gradient to 1 output? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dd4673f",
   "metadata": {},
   "source": [
    "<font size = \"5\" > Section 4.1 - Experiment with other activation functions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4892e40c",
   "metadata": {},
   "source": [
    "This section will be split into several distinct parts. Section 4.1 will go through two different activation functions: softsign and tanhshrink. 4.2 will go through two different opitimzers: adam and adadelta. To compare results, all tests will be done on the original image using the original neural network in section 1. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ceeb003",
   "metadata": {},
   "source": [
    "<font size = \"4\" > Softsign Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the softsign activation function model\n",
    "\n",
    "class CPPN_softsign(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(CPPN_softsign, self).__init__()\n",
    "\n",
    "      # First fully connected layer\n",
    "      self.fc1 = nn.Linear(2, 16)\n",
    "      # Second fully connected layer\n",
    "      self.fc2 = nn.Linear(16, 32)\n",
    "      # Third fully connected layer\n",
    "      self.fc3 = nn.Linear(32, 1)     \n",
    "    \n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass through first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Pass through second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass through third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        # Softsign activation function\n",
    "        x = F.softsign(x)\n",
    "\n",
    "        # Return our output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our model and defining the training epochs and batch size\n",
    "\n",
    "num_epochs = 8\n",
    "batch_size = 1\n",
    "\n",
    "#creating one version using the CPPN class\n",
    "cppns = CPPN_softsign()\n",
    "cppns.to(device)\n",
    "cppns.requires_grad_()\n",
    "\n",
    "#defining the criterion and optimizer to update the weights of the network - using the same optimizer as the original cppn\n",
    "optimizer = torch.optim.SGD(cppns.parameters(), lr=0/1, momentum=0.9)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f297ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "\n",
    "coord_indexes = list(range(0, num_coords))\n",
    "for i in range(int((num_epochs * num_coords) / batch_size)):\n",
    "    optimizer.zero_grad()\n",
    "    cppns.zero_grad()\n",
    "\n",
    "    #get batch of data\n",
    "    batch_indexes = torch.tensor(np.array(random.sample(coord_indexes, batch_size)))\n",
    "    input_batch = X_torch_tensor[batch_indexes]\n",
    "    true_pixel_values = y_torch_tensor[batch_indexes]\n",
    "\n",
    "    #the approx pixel values the model predicts\n",
    "    approx_pixel_values = cppns(input_batch)\n",
    "\n",
    "    #calculate loss function\n",
    "    loss = criterion(true_pixel_values, approx_pixel_values)\n",
    "\n",
    "    if i % 1000 ==0:\n",
    "        print(f'step {i}, loss {loss:.3f}')\n",
    "\n",
    "    #update model and weights \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the whole image with the trained model\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = cppns(X_torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233190a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring and diplaying the results of the color image after it has been trained on the model \n",
    "\n",
    "#reshape the image from 1D to 2D\n",
    "reconstructed_img_cppns = np.reshape(prediction.cpu(), (64, 64))\n",
    "\n",
    "#scale the values from [0,1] to [0,255]\n",
    "reconstructed_img_cppns *= 255\n",
    "\n",
    "#convert the tensor into a nunmpy array, and cast the type into the uint8\n",
    "reconstructed_img_cppns = reconstructed_img_cppns.numpy().astype(np.uint8)\n",
    "\n",
    "#look at our creation next to the original! \n",
    "fig, axes_array = plt.subplots(1, 3, figsize= (20,10))\n",
    "axes_array[0].imshow(img, cmap='gray')\n",
    "axes_array[1].imshow(smaller_img, cmap='gray')\n",
    "axes_array[2].imshow(reconstructed_img_cppns, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e598d8b",
   "metadata": {},
   "source": [
    "What is interesting about this image is that it does recognize the difference in values between light and dark, however, different from the original model, and the model with additional layers, the light and dark values are not oriented correctly to where those values appear on the image. In this production, the darkest values are nearly centered on the image, when in reality, they are associated more towards the left-hand size. This becomes more interesting when thinking about the outputs created between the sigmoid function and the softsign function. Whereas the sigmoid function produces values between 0 and 1, the softsign activation function produces values between -1 and 1. This means the softsign function is 0 centered, and likely to produce a more drastic gradient in color (here color referring to black and white) values than the sigmoid function. This is seen in our produced image as we see a greater depth of color values than the sigmoid function, despite not being as well associated to those values locations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bfb2317",
   "metadata": {},
   "source": [
    "<font size = \"4\"> Tanhshrink Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model using Tanhshrink\n",
    "\n",
    "class CPPN_tanhshrink(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(CPPN_tanhshrink, self).__init__()\n",
    "\n",
    "      # First fully connected layer\n",
    "      self.fc1 = nn.Linear(2, 16)\n",
    "      # Second fully connected layer\n",
    "      self.fc2 = nn.Linear(16, 32)\n",
    "      # Third fully connected layer\n",
    "      self.fc3 = nn.Linear(32, 1)     \n",
    "    \n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass through first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Pass through second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        # Relu activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass through third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        # Softsign activation function\n",
    "        x = F.tanhshrink(x)\n",
    "\n",
    "        # Return our output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb46820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our model and defining the training epochs and batch size\n",
    "\n",
    "num_epochs = 8\n",
    "batch_size = 1\n",
    "\n",
    "#creating one version using the CPPN class\n",
    "cppnt = CPPN_tanhshrink()\n",
    "cppnt.to(device)\n",
    "cppnt.requires_grad_()\n",
    "\n",
    "#defining the criterion and optimizer to update the weights of the network - using the same optimizer as the original cppn\n",
    "optimizer = torch.optim.SGD(cppns.parameters(), lr=0/1, momentum=0.9)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "\n",
    "coord_indexes = list(range(0, num_coords))\n",
    "for i in range(int((num_epochs * num_coords) / batch_size)):\n",
    "    optimizer.zero_grad()\n",
    "    cppnt.zero_grad()\n",
    "\n",
    "    #get batch of data\n",
    "    batch_indexes = torch.tensor(np.array(random.sample(coord_indexes, batch_size)))\n",
    "    input_batch = X_torch_tensor[batch_indexes]\n",
    "    true_pixel_values = y_torch_tensor[batch_indexes]\n",
    "\n",
    "    #the approx pixel values the model predicts\n",
    "    approx_pixel_values = cppnt(input_batch)\n",
    "\n",
    "    #calculate loss function\n",
    "    loss = criterion(true_pixel_values, approx_pixel_values)\n",
    "\n",
    "    if i % 1000 ==0:\n",
    "        print(f'step {i}, loss {loss:.3f}')\n",
    "\n",
    "    #update model and weights \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459256bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the whole image with the trained model\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = cppnt(X_torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring and displaying the image created from training\n",
    "\n",
    "#reshape the image from 1D to 2D\n",
    "reconstructed_img_cppnt = np.reshape(prediction.cpu(), (64, 64))\n",
    "\n",
    "#scale the values from [0,1] to [0,255]\n",
    "reconstructed_img_cppnt *= 255\n",
    "\n",
    "#convert the tensor into a nunmpy array, and cast the type into the uint8\n",
    "reconstructed_img_cppnt = reconstructed_img_cppnt.numpy().astype(np.uint8)\n",
    "\n",
    "#look at our creation next to the original! \n",
    "fig, axes_array = plt.subplots(1, 3, figsize= (20,10))\n",
    "axes_array[0].imshow(img, cmap='gray')\n",
    "axes_array[1].imshow(smaller_img, cmap='gray')\n",
    "axes_array[2].imshow(reconstructed_img_cppnt, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44289c6a",
   "metadata": {},
   "source": [
    "Our Tanhshrink functino appears to have not recognized any light values at all in our original image. This is quite interesting, when looking at the graph produced by the tanhshrink function (available here: https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html) This graph tells us that for every increasing x input, y will also increase (the same is true for negative values). This is quite different from our softsign and sigmoid functions which approach boundaries at -1 and 1 or 0 and 1, respectively. What the produced image tells us, however, is that the tanhshrink activation function (and even suggests any activation function based on tangent) is not a good approach to designing our model.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b419981",
   "metadata": {},
   "source": [
    "<font size = \"5\"> Section 4.2 - Experimenting with a new optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39974f",
   "metadata": {},
   "source": [
    "<font size = \"4\"> Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the Adam optimizer\n",
    "#as I am using the same network structure as Section 1, not everything is copied to this section to avoid redundancy, only what needs to be changed is copied here and changed\n",
    "\n",
    "adam_optimizer = torch.optim.Adam(cppn.parameters(), lr=0/0o1)\n",
    "\n",
    "#training the model \n",
    "coord_indexes = list(range(0, num_coords))\n",
    "for i in range(int((num_epochs * num_coords) / batch_size)):\n",
    "    adam_optimizer.zero_grad()\n",
    "    cppn.zero_grad()\n",
    "\n",
    "    #get batch of data\n",
    "    batch_indexes = torch.tensor(np.array(random.sample(coord_indexes, batch_size)))\n",
    "    input_batch = X_torch_tensor[batch_indexes]\n",
    "    true_pixel_values = y_torch_tensor[batch_indexes]\n",
    "\n",
    "    #the approx pixel values the model predicts\n",
    "    approx_pixel_values = cppn(input_batch)\n",
    "\n",
    "    #calculate loss function\n",
    "    loss = criterion(true_pixel_values, approx_pixel_values)\n",
    "\n",
    "    if i % 1000 ==0:\n",
    "        print(f'step {i}, loss {loss:.3f}')\n",
    "\n",
    "    #update model and weights \n",
    "    loss.backward()\n",
    "    adam_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the whole image with the trained model\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction_adam = cppn(X_torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5345b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the image from 1D to 2D\n",
    "reconstructed_img_adam = np.reshape(prediction_adam.cpu(), (64, 64))\n",
    "\n",
    "#scale the values from [0,1] to [0,255]\n",
    "reconstructed_img_adam *= 255\n",
    "\n",
    "#convert the tensor into a nunmpy array, and cast the type into the uint8\n",
    "reconstructed_img_adam = reconstructed_img_adam.numpy().astype(np.uint8)\n",
    "\n",
    "#look at our creation next to the original! \n",
    "fig, axes_array = plt.subplots(1, 3, figsize= (20,10))\n",
    "axes_array[0].imshow(img, cmap='gray')\n",
    "axes_array[1].imshow(smaller_img, cmap='gray')\n",
    "axes_array[2].imshow(reconstructed_img_adam, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca590c29",
   "metadata": {},
   "source": [
    "The Adam optimizater has produced similar results to the SGD optimization function, only differing in that the black values extend in to the higher y values. When looking at the values in which the darkest values appear, it more closely matches the values on the original image than the image produced using the SGD optimizer. This suggests the Adam optimizer may be a better option for the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31be42b6",
   "metadata": {},
   "source": [
    "<font size = \"4\"> Adadelta Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the adadelta optimizer\n",
    "\n",
    "adadelta_optimizer = torch.optim.Adadelta(cppn.parameters(), lr=1.0)\n",
    "\n",
    "#training the model \n",
    "coord_indexes = list(range(0, num_coords))\n",
    "for i in range(int((num_epochs * num_coords) / batch_size)):\n",
    "    adadelta_optimizer.zero_grad()\n",
    "    cppn.zero_grad()\n",
    "\n",
    "    #get batch of data\n",
    "    batch_indexes = torch.tensor(np.array(random.sample(coord_indexes, batch_size)))\n",
    "    input_batch = X_torch_tensor[batch_indexes]\n",
    "    true_pixel_values = y_torch_tensor[batch_indexes]\n",
    "\n",
    "    #the approx pixel values the model predicts\n",
    "    approx_pixel_values = cppn(input_batch)\n",
    "\n",
    "    #calculate loss function\n",
    "    loss = criterion(true_pixel_values, approx_pixel_values)\n",
    "\n",
    "    if i % 1000 ==0:\n",
    "        print(f'step {i}, loss {loss:.3f}')\n",
    "\n",
    "    #update model and weights \n",
    "    loss.backward()\n",
    "    adadelta_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b726476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the image\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction_ada = cppn(X_torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring and displaying the image\n",
    "\n",
    "#reshape the image from 1D to 2D\n",
    "reconstructed_img_ada = np.reshape(prediction_ada.cpu(), (64, 64))\n",
    "\n",
    "#scale the values from [0,1] to [0,255]\n",
    "reconstructed_img_ada *= 255\n",
    "\n",
    "#convert the tensor into a nunmpy array, and cast the type into the uint8\n",
    "reconstructed_img_ada = reconstructed_img_ada.numpy().astype(np.uint8)\n",
    "\n",
    "#look at our creation next to the original! \n",
    "fig, axes_array = plt.subplots(1, 3, figsize= (20,10))\n",
    "axes_array[0].imshow(img, cmap='gray')\n",
    "axes_array[1].imshow(smaller_img, cmap='gray')\n",
    "axes_array[2].imshow(reconstructed_img_ada, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad774a99",
   "metadata": {},
   "source": [
    "The Adadelta optimizer appears to have given us the best results of the three: Adadelta, Adam, and SGD. In the image produced using the Adadelta optimizer, we have more clearly defined shapes, most easily viewed by the two black triangular shaped corners reaching up to 10 on the y axis, and the other reaching out to 40 on the x axis. These points on the produced image correspond to where the head and arm appear on the smaller and original image. Though the Adam and SGD optimizers gave us similar shapes in this region, neither are as clearly defined as the Adadelta, which suggests the Adadelta may be that much better to use in our model than the Adam or SGD optimizers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06111c8a",
   "metadata": {},
   "source": [
    "<font size= \"5\" > Section 5 - In a paragraph or so, describe how the image we have created differs from the original image. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31912092",
   "metadata": {},
   "source": [
    "The image we have generated across the various tests above differs from our original image in a few ways. One, the image is a recreation of our original image which trained on the color values in our image pixels and then recreated the image based on where it remembers those color values being within the pixelated grid. Second, it has far less pixels in the original image, as we actually trained the network on an image that is 64 x 64 pixels, or the smaller_img saved in various places. Since our produced image was trained on and produces and image with far less pixels than our original image, it is to be expected that the image produced from our network will have far fewer defining characteristics as the original image. These characteristics are everything from the camera stand to facial features. Third, one could argue that the produced image bears no actual connection with the original image. Yes, it was produced by a network trained on the original image. Yes, it bears a resemblance in color delineation on the image (such as the black shape resembling the figure of the man in the original image), but the produced image is a separate entity produced by the computer. The pixels within this produced image are not those within the original image. Thus, it could be classified as a separate entity from the original image, rather than a reproduction of the original image. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "867332fcff6d9ef60802a7aa280ca2e67d2157da374b31f12b9e31ff4c97c0fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
